{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "name='1.jpg'\n",
    "img = cv2.imread('1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def color_balance(img):\n",
    "    # 1. Calculate the average intensity of each color channel\n",
    "    red_avg = np.mean(img[:,:,2])\n",
    "    green_avg = np.mean(img[:,:,1])\n",
    "    blue_avg = np.mean(img[:,:,0])\n",
    "\n",
    "    # 2. Compute the difference between the average intensity and the mean intensity\n",
    "    mean_intensity = np.mean([red_avg, green_avg, blue_avg])\n",
    "    red_diff = mean_intensity - red_avg\n",
    "    green_diff = mean_intensity - green_avg\n",
    "    blue_diff = mean_intensity - blue_avg\n",
    "\n",
    "    # 3. Adjust the intensity of each channel\n",
    "    img[:,:,2] = np.clip(img[:,:,2] + red_diff, 0, 255)\n",
    "    img[:,:,1] = np.clip(img[:,:,1] + green_diff, 0, 255)\n",
    "    img[:,:,0] = np.clip(img[:,:,0] + blue_diff, 0, 255)\n",
    "\n",
    "    # 4. Return the processed image\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "# Apply color balance\n",
    "balanced_img = color_balance(img)\n",
    "\n",
    "variable = name\n",
    "filename = f'balanced_{variable}.jpg'\n",
    "cv2.imwrite(filename, balanced_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Background Light for Each Channel: [107.06115833 107.21775    106.93046667]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def estimate_background_light_for_each_channel(image):\n",
    "    # Initialize background light for each channel\n",
    "    background_light_channels = np.zeros(3)\n",
    "    \n",
    "    # Convert image to grayscale\n",
    "    gray_image = image\n",
    "    \n",
    "    # Initialize threshold for quadtree division\n",
    "    threshold = gray_image.size * 0.001\n",
    "    \n",
    "    # Define function for quadtree division\n",
    "    def quadtree_division(image, threshold):\n",
    "        variance = np.var(image)\n",
    "        \n",
    "        # If variance is less than threshold, return mean intensity as background light\n",
    "        if variance < threshold:\n",
    "            return np.mean(image)\n",
    "        \n",
    "        # Otherwise, divide image into four quadrants\n",
    "        height, width = image.shape\n",
    "        half_height, half_width = height // 2, width // 2\n",
    "        quadrants = [\n",
    "            image[:half_height, :half_width],\n",
    "            image[:half_height, half_width:],\n",
    "            image[half_height:, :half_width],\n",
    "            image[half_height:, half_width:]\n",
    "        ]\n",
    "        \n",
    "        # Recursively call quadtree_division on each quadrant\n",
    "        background_lights = [quadtree_division(quadrant, threshold) for quadrant in quadrants]\n",
    "        \n",
    "        # Return the minimum background light among quadrants\n",
    "        return min(background_lights)\n",
    "    \n",
    "    # Start quadtree division for each channel\n",
    "    for i in range(3):  # Assuming BGR image\n",
    "        background_light_channels[i] = quadtree_division(image[:,:,i], threshold)\n",
    "    \n",
    "    return background_light_channels\n",
    "\n",
    "# Example usage:\n",
    "# Read RGB image\n",
    "new_name = f'balanced_{name}.jpg'\n",
    "image = cv2.imread(new_name)\n",
    "\n",
    "# Estimate background light for each channel\n",
    "background_light_channels = estimate_background_light_for_each_channel(image)\n",
    "print(\"Estimated Background Light for Each Channel:\", background_light_channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_contrast_and_complex_performance_index(I, t):\n",
    "    \n",
    "    # Calculate the number of channels\n",
    "    num_channels = I.shape[2]\n",
    "    \n",
    "    # Initialize arrays to store contrast values Ci for each channel\n",
    "    Ci = np.zeros(num_channels)\n",
    "    \n",
    "    # Loop through each color channel\n",
    "    for i in range(num_channels):\n",
    "       \n",
    "        # Calculate the contrast value Ci for the current channel\n",
    "        squared_diff_channel = (I[:,:,i] - np.mean(I[:,:,i])) ** 2\n",
    "        Ci[i] = np.sum(squared_diff_channel / (t ** 2 * I.size))\n",
    "    \n",
    "    # Calculate the complex contrast performance index Ec\n",
    "    Ec = -np.sum(Ci)\n",
    "    \n",
    "    return Ci, Ec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_information_loss_El(I, A, t):\n",
    "    # Initialize J(x) array\n",
    "    J = np.zeros_like(I)\n",
    "    \n",
    "    # Calculate J(x) using the restoration model equation for each channel\n",
    "    for i in range(3):  # Assuming RGB image\n",
    "\n",
    "        J[:,:,i] = (1 / t) * (I[:,:,i] - A[i]) + A[i]\n",
    "    \n",
    "    # Ensure J is within the allowable range (0 to 255)\n",
    "    J_clipped = np.clip(J, 0, 255)\n",
    "    \n",
    "    # Calculate information loss function El\n",
    "    El = 0\n",
    "    \n",
    "    # Loop through each color channel\n",
    "    for i in range(3):  # Assuming RGB image\n",
    "        # Calculate information loss for each pixel in the channel\n",
    "        loss_channel = (np.minimum(0, J_clipped[:,:,i])**2) + np.maximum(0, J_clipped[:,:,i] - 255)\n",
    "        \n",
    "        # Sum up the information loss for the channel\n",
    "        El += np.sum(loss_channel)\n",
    "    \n",
    "    return El\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyswarms\n",
      "  Downloading pyswarms-1.3.0-py2.py3-none-any.whl.metadata (33 kB)\n",
      "Requirement already satisfied: scipy in .\\.venv\\lib\\site-packages (from pyswarms) (1.13.0)\n",
      "Requirement already satisfied: numpy in .\\.venv\\lib\\site-packages (from pyswarms) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=1.3.1 in .\\.venv\\lib\\site-packages (from pyswarms) (3.8.4)\n",
      "Collecting attrs (from pyswarms)\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting tqdm (from pyswarms)\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting future (from pyswarms)\n",
      "  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting pyyaml (from pyswarms)\n",
      "  Using cached PyYAML-6.0.1-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in .\\.venv\\lib\\site-packages (from matplotlib>=1.3.1->pyswarms) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in .\\.venv\\lib\\site-packages (from matplotlib>=1.3.1->pyswarms) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in .\\.venv\\lib\\site-packages (from matplotlib>=1.3.1->pyswarms) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in .\\.venv\\lib\\site-packages (from matplotlib>=1.3.1->pyswarms) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in .\\.venv\\lib\\site-packages (from matplotlib>=1.3.1->pyswarms) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in .\\.venv\\lib\\site-packages (from matplotlib>=1.3.1->pyswarms) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in .\\.venv\\lib\\site-packages (from matplotlib>=1.3.1->pyswarms) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in .\\.venv\\lib\\site-packages (from matplotlib>=1.3.1->pyswarms) (2.9.0.post0)\n",
      "Requirement already satisfied: colorama in .\\.venv\\lib\\site-packages (from tqdm->pyswarms) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in .\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=1.3.1->pyswarms) (1.16.0)\n",
      "Downloading pyswarms-1.3.0-py2.py3-none-any.whl (104 kB)\n",
      "   ---------------------------------------- 0.0/104.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 104.1/104.1 kB 3.0 MB/s eta 0:00:00\n",
      "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Downloading future-1.0.0-py3-none-any.whl (491 kB)\n",
      "   ---------------------------------------- 0.0/491.3 kB ? eta -:--:--\n",
      "   --------------------------------------- 491.3/491.3 kB 15.5 MB/s eta 0:00:00\n",
      "Using cached PyYAML-6.0.1-cp312-cp312-win_amd64.whl (138 kB)\n",
      "Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, pyyaml, future, attrs, pyswarms\n",
      "Successfully installed attrs-23.2.0 future-1.0.0 pyswarms-1.3.0 pyyaml-6.0.1 tqdm-4.66.2\n"
     ]
    }
   ],
   "source": [
    "! pip install pyswarms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:4158: error: (-215:Assertion failed) !dsize.empty() in function 'cv::resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 58\u001b[0m\n\u001b[0;32m     55\u001b[0m initial_guess \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfull(I\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m], constant)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Optimize transmittance\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m t_optimal, Ec_optimal, El_optimal, E_optimal \u001b[38;5;241m=\u001b[39m \u001b[43moptimize_transmittance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mI\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_L\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_guess\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Print the results\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimized Transmittance (t*):\u001b[39m\u001b[38;5;124m\"\u001b[39m, t_optimal)\n",
      "Cell \u001b[1;32mIn[27], line 19\u001b[0m, in \u001b[0;36moptimize_transmittance\u001b[1;34m(I, A, lambda_L, initial_guess, resize_factor)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize_transmittance\u001b[39m(I, A, lambda_L, initial_guess, resize_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m):\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# Resize the image and the background light\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     I_resized \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(I, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m), fx\u001b[38;5;241m=\u001b[39mresize_factor, fy\u001b[38;5;241m=\u001b[39mresize_factor)\n\u001b[1;32m---> 19\u001b[0m     A_resized \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresize_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresize_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# Adjust the initial guess for t\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     initial_guess_resized \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(initial_guess, (I_resized\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], I_resized\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]))\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:4158: error: (-215:Assertion failed) !dsize.empty() in function 'cv::resize'\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def combined_objective_function(Ec, El, lambda_L):\n",
    "    # Calculate the combined objective function\n",
    "    return Ec + lambda_L * El\n",
    "\n",
    "def optimize_transmittance(I, A, lambda_L, initial_guesses):\n",
    "    # Initialize lists to store results\n",
    "    optimal_t = []\n",
    "    optimal_Ec = []\n",
    "    optimal_El = []\n",
    "    optimal_E = []\n",
    "    print(len(initial_guesses),\"kk\")\n",
    "    # Loop through each initial guess for t\n",
    "    for i, initial_guess in enumerate(initial_guesses):\n",
    "       \n",
    "\n",
    "       \n",
    "            # Calculate contrast value Ci and complex contrast performance index Ec\n",
    "        Ci, Ec = calculate_contrast_and_complex_performance_index(I, initial_guess)\n",
    "    \n",
    "            # Calculate information loss El\n",
    "        El = calculate_information_loss_El(I, A, initial_guess)\n",
    "            \n",
    "            # Calculate combined objective function E\n",
    "        E = combined_objective_function(Ec, El, lambda_L)\n",
    "            \n",
    "            # Store results\n",
    "        \n",
    "                \n",
    "        \n",
    "        \n",
    "        # Store the results for this initial guess\n",
    "        optimal_t.append(initial_guess)\n",
    "        optimal_Ec.append(Ec)\n",
    "        optimal_El.append(El)\n",
    "        optimal_E.append(E)\n",
    "    \n",
    "    # Find the index corresponding to the minimum value of E among all initial guesses\n",
    "    best_index = np.argmin(optimal_E)\n",
    "    \n",
    "    # Return the best results\n",
    "    return optimal_t[best_index], optimal_Ec[best_index], optimal_El[best_index], optimal_E[best_index]\n",
    "\n",
    "# Example usage:\n",
    "# Read RGB image\n",
    "I = cv2.imread(new_name)\n",
    "\n",
    "# Background light A obtained from Step 1 (assuming some value)\n",
    "A = background_light_channels\n",
    "\n",
    "# Tradeoff factor\n",
    "lambda_L = 5\n",
    "\n",
    "# Initial guesses for transmittance\n",
    "constant = 0.1  # or any small value you prefer\n",
    "initial_guesses = [np.around(np.clip(np.random.rand(*I.shape[:2]) + constant, constant,1), decimals=4) for _ in range(10000)]\n",
    "\n",
    "# Optimize transmittance\n",
    "t_optimal, Ec_optimal, El_optimal, E_optimal = optimize_transmittance(I, A, lambda_L,initial_guesses )\n",
    "\n",
    "# Print the results\n",
    "print(\"Optimized Transmittance (t*):\", t_optimal)\n",
    "print(\"Optimal Ec:\", Ec_optimal)\n",
    "print(\"Optimal El:\", El_optimal)\n",
    "print(\"Optimal E:\", E_optimal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_transmittance(I, A, lambda_L, initial_guess, resize_factor=0.5):\n",
    "    # Resize the image\n",
    "    I_resized = cv2.resize(I, (0, 0), fx=resize_factor, fy=resize_factor)\n",
    "\n",
    "    # Adjust the initial guess for t\n",
    "    initial_guess_resized = cv2.resize(initial_guess, (I_resized.shape[1], I_resized.shape[0]))\n",
    "\n",
    "    # Flatten the initial guess into a 1D array\n",
    "    initial_guess_flat = initial_guess_resized.flatten()\n",
    "\n",
    "    # Use scipy.optimize.minimize to find the optimal t\n",
    "    result = minimize(combined_objective_function, initial_guess_flat, args=(I_resized, A, lambda_L), method='Nelder-Mead')\n",
    "\n",
    "    # Reshape the optimal t back into a 2D matrix\n",
    "    t_optimal_resized = result.x.reshape(I_resized.shape[:2])\n",
    "\n",
    "    # Resize the optimized t back to the original size\n",
    "    t_optimal = cv2.resize(t_optimal_resized, (I.shape[1], I.shape[0]))\n",
    "\n",
    "    # Calculate the optimal Ec, El, and E\n",
    "    Ci_optimal, Ec_optimal = calculate_contrast_and_complex_performance_index(I, t_optimal)\n",
    "    El_optimal = calculate_information_loss_El(I, A, t_optimal)\n",
    "    E_optimal = combined_objective_function(t_optimal, I, A, lambda_L)\n",
    "\n",
    "    return t_optimal, Ec_optimal, El_optimal, E_optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_guess = np.around(np.clip(np.random.rand(*I.shape[:2]) + constant, constant,1), decimals=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def restore_image(I, A, t):\n",
    "    # Initialize the restored image\n",
    "    restored_image = np.zeros_like(I)\n",
    "\n",
    "    # Iterate over each pixel in the image\n",
    "    for i in range(I.shape[0]):\n",
    "        for j in range(I.shape[1]):\n",
    "            # Apply the restoration model formula for each channel\n",
    "            for k in range(3):  # Iterate over R, G, B channels\n",
    "                restored_image[i, j, k] = (1 / t[i, j]) * (I[i, j, k] - A[k]) + A[k]\n",
    "\n",
    "    return restored_image\n",
    "\n",
    "# Example usage:\n",
    "# Read RGB image\n",
    "I = cv2.imread(new_name)\n",
    "\n",
    "# Assuming background light A is already obtained\n",
    "A = background_light_channels # Example background light (for each channel)\n",
    "\n",
    "\n",
    "# Restore the image using the restoration model and the calculated transmittance map\n",
    "\n",
    "restored_image = restore_image(I, A, t_optimal)\n",
    "restored_name=f'restored_{name}.jpg'\n",
    "cv2.imwrite(restored_name, restored_image)\n",
    "# Display or save the restored image\n",
    "cv2.imshow('Restored Image', restored_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[170 164 104]\n",
      "  [180 173 103]\n",
      "  [153 148 104]\n",
      "  ...\n",
      "  [158 147  85]\n",
      "  [216 192  59]\n",
      "  [170 156  79]]\n",
      "\n",
      " [[ 15 255  99]\n",
      "  [160 155 104]\n",
      "  [194 186 103]\n",
      "  ...\n",
      "  [196 177  68]\n",
      "  [247 217  46]\n",
      "  [163 151  82]]\n",
      "\n",
      " [[174 168 103]\n",
      "  [160 155 104]\n",
      "  [189 181 103]\n",
      "  ...\n",
      "  [ 93  40   2]\n",
      "  [136  74 240]\n",
      "  [158 147  85]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[129 202  75]\n",
      "  [121 168 252]\n",
      "  [130 208  89]\n",
      "  ...\n",
      "  [ 62  95 112]\n",
      "  [ 74  97 108]\n",
      "  [ 18  76 107]]\n",
      "\n",
      " [[100 120 145]\n",
      "  [101 123 150]\n",
      "  [ 92 182  39]\n",
      "  ...\n",
      "  [ 64 103 123]\n",
      "  [255  90 138]\n",
      "  [ 87 103 110]]\n",
      "\n",
      " [[ 91 124 165]\n",
      "  [ 99 115 134]\n",
      "  [100 121 148]\n",
      "  ...\n",
      "  [ 77 106 121]\n",
      "  [ 26  94 130]\n",
      "  [229  86 145]]]\n"
     ]
    }
   ],
   "source": [
    "print(restored_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2;\n",
    "# import math;\n",
    "# import numpy as np;\n",
    "\n",
    "# def DarkChannel(im,sz):\n",
    "#     b,g,r = cv2.split(im)\n",
    "#     dc = cv2.min(cv2.min(r,g),b);\n",
    "#     kernel = cv2.getStructuringElement(cv2.MORPH_RECT,(sz,sz))\n",
    "#     dark = cv2.erode(dc,kernel)\n",
    "#     return dark\n",
    "\n",
    "# def AtmLight(im,dark):\n",
    "#     [h,w] = im.shape[:2]\n",
    "#     imsz = h*w\n",
    "#     numpx = int(max(math.floor(imsz/1000),1))\n",
    "#     darkvec = dark.reshape(imsz);\n",
    "#     imvec = im.reshape(imsz,3);\n",
    "\n",
    "#     indices = darkvec.argsort();\n",
    "#     indices = indices[imsz-numpx::]\n",
    "\n",
    "#     atmsum = np.zeros([1,3])\n",
    "#     for ind in range(1,numpx):\n",
    "#        atmsum = atmsum + imvec[indices[ind]]\n",
    "\n",
    "#     A = atmsum / numpx;\n",
    "#     return A\n",
    "\n",
    "# def TransmissionEstimate(im,A,sz):\n",
    "#     omega = 0.95;\n",
    "#     im3 = np.empty(im.shape,im.dtype);\n",
    "\n",
    "#     for ind in range(0,3):\n",
    "#         im3[:,:,ind] = im[:,:,ind]/A[0,ind]\n",
    "\n",
    "#     transmission = 1 - omega*DarkChannel(im3,sz);\n",
    "#     return transmission\n",
    "\n",
    "# def Guidedfilter(im,p,r,eps):\n",
    "#     mean_I = cv2.boxFilter(im,cv2.CV_64F,(r,r));\n",
    "#     mean_p = cv2.boxFilter(p, cv2.CV_64F,(r,r));\n",
    "#     mean_Ip = cv2.boxFilter(im*p,cv2.CV_64F,(r,r));\n",
    "#     cov_Ip = mean_Ip - mean_I*mean_p;\n",
    "\n",
    "#     mean_II = cv2.boxFilter(im*im,cv2.CV_64F,(r,r));\n",
    "#     var_I   = mean_II - mean_I*mean_I;\n",
    "\n",
    "#     a = cov_Ip/(var_I + eps);\n",
    "#     b = mean_p - a*mean_I;\n",
    "\n",
    "#     mean_a = cv2.boxFilter(a,cv2.CV_64F,(r,r));\n",
    "#     mean_b = cv2.boxFilter(b,cv2.CV_64F,(r,r));\n",
    "\n",
    "#     q = mean_a*im + mean_b;\n",
    "#     return q;\n",
    "\n",
    "# def TransmissionRefine(im,et):\n",
    "#     gray = cv2.cvtColor(im,cv2.COLOR_BGR2GRAY);\n",
    "#     gray = np.float64(gray)/255;\n",
    "#     r = 60;\n",
    "#     eps = 0.0001;\n",
    "#     t = Guidedfilter(gray,et,r,eps);\n",
    "\n",
    "#     return t;\n",
    "\n",
    "# def Recover(im,t,A,tx = 0.1):\n",
    "#     res = np.empty(im.shape,im.dtype);\n",
    "#     t = cv2.max(t,tx);\n",
    "\n",
    "#     for ind in range(0,3):\n",
    "#         res[:,:,ind] = (im[:,:,ind]-A[0,ind])/t + A[0,ind]\n",
    "\n",
    "#     return res\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     import sys\n",
    "#     try:\n",
    "#         fn = sys.argv[1]\n",
    "#     except:\n",
    "#         fn = './image/15.png'\n",
    "\n",
    "#     def nothing(*argv):\n",
    "#         pass\n",
    "\n",
    "#     src = cv2.imread(\"balanced.jpg\");\n",
    "\n",
    "#     I = src.astype('float64')/255;\n",
    " \n",
    "#     dark = DarkChannel(I,15);\n",
    "#     A = AtmLight(I,dark);\n",
    "#     te = TransmissionEstimate(I,A,15);\n",
    "#     t = TransmissionRefine(src,te);\n",
    "#     dcb = Recover(I,t,A,0.1);\n",
    "\n",
    "  \n",
    "    \n",
    "#     cv2.imwrite(\"dcbimage.jpg\",dcb*255);\n",
    "#     cv2.waitKey();\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "\n",
    "# def denormalize_image(image):\n",
    "#     # Denormalize image from range [0, 1] to [0, 255]\n",
    "#     image = image * 255\n",
    "\n",
    "#     # Convert image to uint8\n",
    "#     image = image.astype(np.uint8)\n",
    "\n",
    "#     return image\n",
    "# dcb=denormalize_image(dcb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def calculate_average_red_channel_value(image):\n",
    "#     # Assuming 'image' is a 3D NumPy array representing the image (height x width x channels)\n",
    "#     red_channel = image[:, :, 0]  # Assuming red channel is the first channel (index 0)\n",
    "#     average_red_value = np.mean(red_channel)\n",
    "#     return average_red_value\n",
    "\n",
    "# average_red = calculate_average_red_channel_value(dcb)\n",
    "# print(\"Average Red Channel Value:\", average_red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Red Channel Value: 109.21555\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_average_red_channel_value(image):\n",
    "    # Assuming 'image' is a 3D NumPy array representing the image (height x width x channels)\n",
    "    red_channel = image[:, :, 0]  # Assuming red channel is the first channel (index 0)\n",
    "    average_red_value = np.mean(red_channel)\n",
    "    return average_red_value\n",
    "\n",
    "average_red = calculate_average_red_channel_value(restored_image)\n",
    "print(\"Average Red Channel Value:\", average_red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120000\n",
      "Histogram Threshold: 138.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def calculate_histogram_threshold(image):\n",
    "    # Calculate total number of pixels\n",
    "    height, width, _ = image.shape\n",
    "    total_pixels = height * width\n",
    "    print(total_pixels)\n",
    "    # Calculate histogram threshold (h1) as n*0.225%\n",
    "    histogram_threshold = total_pixels * 0.00115\n",
    "    \n",
    "    return  histogram_threshold\n",
    "\n",
    "histogram_threshold = calculate_histogram_threshold(restored_image)\n",
    "print(\"Histogram Threshold:\", histogram_threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r_threshold = 107\n",
    "# def calculate_histogram_threshold(image):\n",
    "#     # Calculate total number of pixels\n",
    "#     height, width, _ = image.shape\n",
    "#     total_pixels = height * width\n",
    "#     print(total_pixels)\n",
    "#     # Calculate histogram threshold (h1) as n*0.225%\n",
    "#     histogram_threshold = total_pixels * 0.00115\n",
    "    \n",
    "#     return  histogram_threshold\n",
    "\n",
    "# histogram_threshold = calculate_histogram_threshold(dcb)\n",
    "# print(\"Histogram Threshold:\", histogram_threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def histogram_stretching(image, R_ave, R_threshold):\n",
    "#     # Calculate total number of pixels\n",
    "#     height, width, _ = image.shape\n",
    "#     total_pixels = height * width\n",
    "    \n",
    "#     # Determine if attenuation of red light is slight or heavy\n",
    "#     if R_ave >= R_threshold:  # Slight attenuation\n",
    "#         channels = [0, 1, 2]  # Stretching for all channels (R, G, B)\n",
    "#     else:  # Heavy attenuation\n",
    "#         channels = [1, 2]  # Stretching for G, B channels only\n",
    "    \n",
    "#     # Calculate satisfactory threshold ht as n * 0.225%\n",
    "#     ht = total_pixels * 0.00225\n",
    "    \n",
    "#     # Calculate minimal and maximal scalar values based on ht\n",
    "#     imin = 0\n",
    "#     imax = 255\n",
    "    \n",
    "#     # If ht is provided and it's greater than zero, update imin and imax\n",
    "#     if ht > 0:\n",
    "#         # Calculate lower and upper thresholds based on ht\n",
    "#         imin = np.min(image)\n",
    "#         imax = np.max(image)\n",
    "#         lower_threshold_count = int(ht)\n",
    "#         upper_threshold_count = int(total_pixels - ht)\n",
    "#         sorted_pixels = np.sort(image.flatten())\n",
    "#         imin = sorted_pixels[lower_threshold_count]\n",
    "#         imax = sorted_pixels[upper_threshold_count]\n",
    "\n",
    "#     # Apply histogram stretching to selected channels\n",
    "#     stretched_image = image.copy()\n",
    "#     for channel in channels:\n",
    "#         for i in range(height):\n",
    "#             for j in range(width):\n",
    "#                 old_value = stretched_image[i, j, channel]\n",
    "#                 if old_value < imin:\n",
    "#                     stretched_image[i, j, channel] = 0\n",
    "#                 elif old_value > imax:\n",
    "#                     stretched_image[i, j, channel] = 255\n",
    "#                 else:\n",
    "#                     stretched_image[i, j, channel] = 255 * (old_value - imin) / (imax - imin)\n",
    "    \n",
    "#     return stretched_image\n",
    "\n",
    "# # Example usage:\n",
    "# stretched_image = histogram_stretching(dcb, average_red, r_threshold)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "r_threshold = 0\n",
    "\n",
    "def histogram_stretching(image, R_ave, R_threshold):\n",
    "    # Calculate total number of pixels\n",
    "    height, width, _ = image.shape\n",
    "    total_pixels = height * width\n",
    "    \n",
    "    # Determine if attenuation of red light is slight or heavy\n",
    "    if R_ave >= R_threshold:  # Slight attenuation\n",
    "        channels = [0, 1, 2]  # Stretching for all channels (R, G, B)\n",
    "    else:  # Heavy attenuation\n",
    "        channels = [1, 2]  # Stretching for G, B channels only\n",
    "    \n",
    "    # Calculate satisfactory threshold ht as n * 0.225%\n",
    "    ht = total_pixels * 0.00225\n",
    "    \n",
    "    # Calculate minimal and maximal scalar values based on ht\n",
    "    imin = 0\n",
    "    imax = 255\n",
    "    \n",
    "    # If ht is provided and it's greater than zero, update imin and imax\n",
    "    if ht > 0:\n",
    "        # Calculate lower and upper thresholds based on ht\n",
    "        imin = np.min(image)\n",
    "        imax = np.max(image)\n",
    "        lower_threshold_count = int(ht)\n",
    "        upper_threshold_count = int(total_pixels - ht)\n",
    "        sorted_pixels = np.sort(image.flatten())\n",
    "        imin = sorted_pixels[lower_threshold_count]\n",
    "        imax = sorted_pixels[upper_threshold_count]\n",
    "\n",
    "    # Apply histogram stretching to selected channels\n",
    "    stretched_image = image.copy()\n",
    "    for channel in channels:\n",
    "        for i in range(height):\n",
    "            for j in range(width):\n",
    "                old_value = stretched_image[i, j, channel]\n",
    "                if old_value < imin:\n",
    "                    stretched_image[i, j, channel] = 0\n",
    "                elif old_value > imax:\n",
    "                    stretched_image[i, j, channel] = 255\n",
    "                else:\n",
    "                    stretched_image[i, j, channel] = 255 * (old_value - imin) / (imax - imin)\n",
    "    \n",
    "    return stretched_image\n",
    "\n",
    "# Example usage:\n",
    "stretched_image_model = histogram_stretching(restored_image, average_red, r_threshold)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streched_name=f'stretched_{name}.jpg'\n",
    "cv2.imwrite(streched_name, stretched_image_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage import color\n",
    "\n",
    "def calculate_rms_contrast(image):\n",
    "    \"\"\"\n",
    "    Calculate the Root Mean Square (RMS) contrast of an image.\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.mean(np.square(image - np.mean(image))))\n",
    "\n",
    "def calculate_uciqe(image):\n",
    "    \"\"\"\n",
    "    Calculate the Underwater Color Image Quality Evaluation (UCIQE) of an image.\n",
    "    \"\"\"\n",
    "    c1 = 0.4680\n",
    "    c2 = 0.2745\n",
    "    c3 = 0.2576\n",
    "    img_yuv = color.rgb2yuv(image)\n",
    "    y = img_yuv[:,:,0]\n",
    "    u = img_yuv[:,:,1]\n",
    "    v = img_yuv[:,:,2]\n",
    "    uciqe = c1 * np.std(y) / np.mean(y) + c2 * np.cov(u, v) / (np.std(u) * np.std(v)) + c3 * np.mean(v)\n",
    "    return uciqe\n",
    "\n",
    "def calculate_psnr(image_true, image_test):\n",
    "    \"\"\"\n",
    "    Calculate the Peak Signal-to-Noise Ratio (PSNR) between two images.\n",
    "    \"\"\"\n",
    "    return psnr(image_true, image_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMS contrast of image 1: 15.576318265089562\n",
      "RMS contrast of image 2: 44.177682355259584\n",
      "UCIQE of image 1: [[0.24534417 0.24535076 0.24419867 ... 0.10883901 0.10980822 0.10996243]\n",
      " [0.24535076 0.2453794  0.24422585 ... 0.10888598 0.10985345 0.11000683]\n",
      " [0.24419867 0.24422585 0.24330792 ... 0.10905831 0.11003872 0.11017365]\n",
      " ...\n",
      " [0.10883901 0.10888598 0.10905831 ... 0.09381207 0.09287067 0.09304693]\n",
      " [0.10980822 0.10985345 0.11003872 ... 0.09287067 0.09428684 0.09445564]\n",
      " [0.10996243 0.11000683 0.11017365 ... 0.09304693 0.09445564 0.09474101]]\n",
      "UCIQE of image 2: [[0.28078693 0.13873432 0.13761515 ... 0.13363631 0.12608118 0.09595774]\n",
      " [0.13873432 0.30969605 0.14814454 ... 0.12780133 0.11453609 0.11636461]\n",
      " [0.13761515 0.14814454 0.22522155 ... 0.14646533 0.12093957 0.11049589]\n",
      " ...\n",
      " [0.13363631 0.12780133 0.14646533 ... 0.43069363 0.14025583 0.11586494]\n",
      " [0.12608118 0.11453609 0.12093957 ... 0.14025583 0.5266953  0.12699541]\n",
      " [0.09595774 0.11636461 0.11049589 ... 0.11586494 0.12699541 0.51003115]]\n",
      "PSNR between image 1 and image 2: 5.598958542513196\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "win_size exceeds image extent. Either ensure that your images are at least 7x7; or pass win_size explicitly in the function call, with an odd value less than or equal to the smaller side of your images. If your images are multichannel (with color channels), set channel_axis to the axis number corresponding to the channels.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 26\u001b[0m\n\u001b[0;32m     22\u001b[0m     ssim_value \u001b[38;5;241m=\u001b[39m calculate_ssim(image1, image2)\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSSIM between image 1 and image 2: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mssim_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m \u001b[43mfuu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstretched_image_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[47], line 22\u001b[0m, in \u001b[0;36mfuu\u001b[1;34m(image1, image2)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPSNR between image 1 and image 2: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpsnr_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Calculate SSIM\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m ssim_value \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_ssim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSSIM between image 1 and image 2: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mssim_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[41], line 45\u001b[0m, in \u001b[0;36mcalculate_ssim\u001b[1;34m(image_true, image_test)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# If window size is less than 7, set it to the minimum valid value of 7\u001b[39;00m\n\u001b[0;32m     43\u001b[0m win_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(win_size, \u001b[38;5;241m7\u001b[39m)\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwin_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwin_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultichannel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\vit\\sem6\\dip\\challenging-60\\.venv\\Lib\\site-packages\\skimage\\metrics\\_structural_similarity.py:186\u001b[0m, in \u001b[0;36mstructural_similarity\u001b[1;34m(im1, im2, win_size, gradient, data_range, channel_axis, gaussian_weights, full, **kwargs)\u001b[0m\n\u001b[0;32m    183\u001b[0m         win_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7\u001b[39m  \u001b[38;5;66;03m# backwards compatibility\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many((np\u001b[38;5;241m.\u001b[39masarray(im1\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m-\u001b[39m win_size) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwin_size exceeds image extent. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    188\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEither ensure that your images are \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    189\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mat least 7x7; or pass win_size explicitly \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    190\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124min the function call, with an odd value \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    191\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mless than or equal to the smaller side of your \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages. If your images are multichannel \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    193\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(with color channels), set channel_axis to \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe axis number corresponding to the channels.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    195\u001b[0m     )\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (win_size \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWindow size must be odd.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: win_size exceeds image extent. Either ensure that your images are at least 7x7; or pass win_size explicitly in the function call, with an odd value less than or equal to the smaller side of your images. If your images are multichannel (with color channels), set channel_axis to the axis number corresponding to the channels."
     ]
    }
   ],
   "source": [
    "def fuu(image1, image2):\n",
    "    \"\"\"\n",
    "    Main function to calculate image quality metrics.\n",
    "    \"\"\"\n",
    "    # Calculate RMS contrast\n",
    "    rms_contrast1 = calculate_rms_contrast(image1)\n",
    "    rms_contrast2 = calculate_rms_contrast(image2)\n",
    "    print(f\"RMS contrast of image 1: {rms_contrast1}\")\n",
    "    print(f\"RMS contrast of image 2: {rms_contrast2}\")\n",
    "\n",
    "    # Calculate UCIQE\n",
    "    uciqe1 = calculate_uciqe(image1)\n",
    "    uciqe2 = calculate_uciqe(image2)\n",
    "    print(f\"UCIQE of image 1: {uciqe1}\")\n",
    "    print(f\"UCIQE of image 2: {uciqe2}\")\n",
    "\n",
    "    # Calculate PSNR\n",
    "    psnr_value = calculate_psnr(image1, image2)\n",
    "    print(f\"PSNR between image 1 and image 2: {psnr_value}\")\n",
    "\n",
    "   \n",
    "\n",
    "fuu(img, stretched_image_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
